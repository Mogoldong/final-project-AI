"""
ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬
ë ˆì‹œí”¼ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ (ì„ íƒì‚¬í•­)
"""

from bs4 import BeautifulSoup
import requests
from typing import List, Dict
import time


class RecipeScraper:
    """ë ˆì‹œí”¼ ì›¹ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self):
        """ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def scrape_recipe(self, url: str) -> Dict:
        """
        ë‹¨ì¼ ë ˆì‹œí”¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        Args:
            url: ë ˆì‹œí”¼ í˜ì´ì§€ URL

        Returns:
            ë ˆì‹œí”¼ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ê¸°ë³¸ ë ˆì‹œí”¼ êµ¬ì¡° (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            recipe = {
                "title": self._extract_title(soup),
                "ingredients": self._extract_ingredients(soup),
                "steps": self._extract_steps(soup),
                "cooking_time": self._extract_cooking_time(soup),
                "difficulty": self._extract_difficulty(soup),
                "cuisine_type": "ì •ë³´ ì—†ìŒ",
                "servings": self._extract_servings(soup),
                "calories": "ì •ë³´ ì—†ìŒ",
                "source_url": url
            }

            return recipe

        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì œëª© íƒœê·¸ë“¤ì„ ì‹œë„
        title_tags = soup.find_all(['h1', 'h2'], class_=['title', 'recipe-title', 'heading'])
        if title_tags:
            return title_tags[0].get_text(strip=True)
        return "ì œëª© ì—†ìŒ"

    def _extract_ingredients(self, soup: BeautifulSoup) -> List[str]:
        """ì¬ë£Œ ëª©ë¡ ì¶”ì¶œ"""
        ingredients = []

        # ì¼ë°˜ì ì¸ ì¬ë£Œ ëª©ë¡ íŒ¨í„´
        ingredient_sections = soup.find_all(['ul', 'ol'], class_=['ingredients', 'ingredient-list'])

        for section in ingredient_sections:
            items = section.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if text:
                    ingredients.append(text)

        return ingredients

    def _extract_steps(self, soup: BeautifulSoup) -> List[str]:
        """ì¡°ë¦¬ ë‹¨ê³„ ì¶”ì¶œ"""
        steps = []

        # ì¼ë°˜ì ì¸ ì¡°ë¦¬ ë‹¨ê³„ íŒ¨í„´
        step_sections = soup.find_all(['ol', 'ul'], class_=['steps', 'instructions', 'directions'])

        for section in step_sections:
            items = section.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if text:
                    steps.append(text)

        return steps

    def _extract_cooking_time(self, soup: BeautifulSoup) -> str:
        """ì¡°ë¦¬ ì‹œê°„ ì¶”ì¶œ"""
        time_tags = soup.find_all(['span', 'div'], class_=['time', 'cooking-time', 'prep-time'])
        if time_tags:
            return time_tags[0].get_text(strip=True)
        return "ì •ë³´ ì—†ìŒ"

    def _extract_difficulty(self, soup: BeautifulSoup) -> str:
        """ë‚œì´ë„ ì¶”ì¶œ"""
        difficulty_tags = soup.find_all(['span', 'div'], class_=['difficulty', 'level'])
        if difficulty_tags:
            return difficulty_tags[0].get_text(strip=True)
        return "ì •ë³´ ì—†ìŒ"

    def _extract_servings(self, soup: BeautifulSoup) -> str:
        """ì¸ë¶„ ìˆ˜ ì¶”ì¶œ"""
        serving_tags = soup.find_all(['span', 'div'], class_=['servings', 'yield'])
        if serving_tags:
            return serving_tags[0].get_text(strip=True)
        return "ì •ë³´ ì—†ìŒ"

    def scrape_multiple_recipes(self, urls: List[str], delay: float = 1.0) -> List[Dict]:
        """
        ì—¬ëŸ¬ ë ˆì‹œí”¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        Args:
            urls: ë ˆì‹œí”¼ í˜ì´ì§€ URL ë¦¬ìŠ¤íŠ¸
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        Returns:
            ë ˆì‹œí”¼ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        recipes = []

        for i, url in enumerate(urls, 1):
            print(f"ğŸ” ìŠ¤í¬ë˜í•‘ ì¤‘... ({i}/{len(urls)}): {url}")

            recipe = self.scrape_recipe(url)

            if recipe:
                recipes.append(recipe)

            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if i < len(urls):
                time.sleep(delay)

        print(f"âœ… ì´ {len(recipes)}ê°œì˜ ë ˆì‹œí”¼ë¥¼ ìŠ¤í¬ë˜í•‘í–ˆìŠµë‹ˆë‹¤.")
        return recipes

    def save_to_json(self, recipes: List[Dict], output_file: str):
        """
        ìŠ¤í¬ë˜í•‘í•œ ë ˆì‹œí”¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

        Args:
            recipes: ë ˆì‹œí”¼ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            output_file: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        import json

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(recipes, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ ë ˆì‹œí”¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    """
    ì‚¬ìš© ì˜ˆì‹œ:

    scraper = RecipeScraper()

    # ë‹¨ì¼ ë ˆì‹œí”¼ ìŠ¤í¬ë˜í•‘
    recipe = scraper.scrape_recipe("https://example.com/recipe/kimchi-jjigae")

    # ì—¬ëŸ¬ ë ˆì‹œí”¼ ìŠ¤í¬ë˜í•‘
    urls = [
        "https://example.com/recipe/1",
        "https://example.com/recipe/2",
    ]
    recipes = scraper.scrape_multiple_recipes(urls)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    scraper.save_to_json(recipes, "./data/raw/scraped_recipes.json")
    """
    print("RecipeScraper í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")
    print("ìì„¸í•œ ì‚¬ìš©ë²•ì€ ì£¼ì„ì„ ì°¸ê³ í•˜ì„¸ìš”.")
