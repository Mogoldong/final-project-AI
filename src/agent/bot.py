import os
import json
from typing import TypedDict, Annotated, List, Literal, Generator, Dict, Any
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt, Command

from src.agent.tool_registry import ToolRegistry, register_default_tools
from src.agent.memory_extractor import extract_and_save_memory

load_dotenv()


# LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•œë‹¤
class AgentState(TypedDict):
    messages: Annotated[List, add_messages]
    google_search_count: int


class LangGraphAgent:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.registry = register_default_tools()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(model=model, api_key=self.api_key, temperature=0, streaming=True)
        self.tools_schema = self.registry.list_openai_tools()
        self.llm_with_tools = self.llm.bind_tools(self.tools_schema)

        self.system_prompt = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìžì˜ ìƒí™©ê³¼ ê¸°ë¶„ì— ë§žì¶° ìš”ë¦¬ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” AI ì…°í”„ë´‡ìž…ë‹ˆë‹¤.
        - ì‚¬ìš©ìžì˜ ì·¨í–¥ì´ë‚˜ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ê¸°ì–µ(read_memory)í•˜ê³  í™œìš©í•˜ì„¸ìš”.
        - RAG(ë ˆì‹œí”¼/ì§€ì‹ ê²€ìƒ‰)ì— ì •ë³´ê°€ ì—†ê±°ë‚˜, ìž¬ë£Œ ëŒ€ì²´ë²• ë“± ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ìžˆìœ¼ë©´ 'êµ¬ê¸€ ê²€ìƒ‰' íˆ´ì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        - í•­ìƒ ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        self.graph = self._build_graph()

    def call_model(self, state: AgentState):
        messages = state["messages"]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=self.system_prompt)] + messages
            
        response = self.llm_with_tools.invoke(messages)
        
        return {"messages": [response]}

    def run_tools(self, state: AgentState):
        last_message = state["messages"][-1]
        tool_calls = last_message.tool_calls
        
        results = []
        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_id = tool_call["id"]
            
            try:
                tool_output = self.registry.call(tool_name, tool_args)
            except Exception as e:
                tool_output = f"Error: {str(e)}"

            content = json.dumps(tool_output, ensure_ascii=False)

            results.append(ToolMessage(
                tool_call_id=tool_id,
                name=tool_name,
                content=content
            ))
            
        google_search_count = state.get("google_search_count", 0)
        search_count_in_turn = sum(1 for msg in results if msg.name == 'search_google')
        
        return {"messages": results, "google_search_count": google_search_count + search_count_in_turn}

    def should_continue(self, state: AgentState) -> Literal["tools", END]:
        last_message = state["messages"][-1]
        
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"
        return END
    
    def check_interrupt(self, state: AgentState):
        """
        ì¸í„°ëŸ½íŠ¸ ì²´í¬ ë…¸ë“œ
        - ê²€ìƒ‰ íšŸìˆ˜ê°€ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´ interrupt() í˜¸ì¶œ
        """
        current_count = state.get("google_search_count", 0)
        
        # ê²€ìƒ‰ íšŸìˆ˜ê°€ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´ interrupt ë°œìƒ
        if current_count > 3:
            # interrupt()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ìž ìž…ë ¥ì„ ë°›ìŒ
            user_input = interrupt(
                f"ðŸš¨ Google ê²€ìƒ‰ í•œë„ ì´ˆê³¼ ì•Œë¦¼\n\n"
                f"í˜„ìž¬ {current_count}íšŒì˜ ê²€ìƒ‰ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (ê¶Œìž¥: 3íšŒ)\n"
                f"í•˜ë£¨ API í˜¸ì¶œ í•œë„ëŠ” 100íšŒìž…ë‹ˆë‹¤.\n\n"
                f"ê³„ì† ê²€ìƒ‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
            )
            
            # ì‚¬ìš©ìž ì‘ë‹µì´ ìžˆëŠ” ê²½ìš° ì²˜ë¦¬
            if user_input:
                user_response = str(user_input).strip().lower()
                
                # ì‚¬ìš©ìžê°€ ê³„ì† ì§„í–‰ì„ ì„ íƒí•œ ê²½ìš°
                if user_response in ["continue", "yes", "ë„¤", "ê³„ì†", "y", "ã…‡ã…‡", "ì‘", "ok"]:
                    return {"messages": [SystemMessage(
                        content="[ì‹œìŠ¤í…œ] ì‚¬ìš©ìžê°€ ê²€ìƒ‰ ê³„ì† ì§„í–‰ì„ ìŠ¹ì¸í–ˆìŠµë‹ˆë‹¤."
                    )]}
                else:
                    # ì¤‘ë‹¨ì„ ì„ íƒí•œ ê²½ìš°
                    return {"messages": [SystemMessage(
                        content="[ì‹œìŠ¤í…œ] ì‚¬ìš©ìžê°€ ê²€ìƒ‰ ì¤‘ë‹¨ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. í˜„ìž¬ ì •ë³´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
                    )]}
        
        # ì •ìƒ ì§„í–‰
        return {"messages": []}

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", self.run_tools)
        workflow.add_node("check_interrupt", self.check_interrupt)

        workflow.set_entry_point("agent")
        
        # agent â†’ tools ë˜ëŠ” END
        workflow.add_conditional_edges(
            "agent",
            self.should_continue,
            {
                "tools": "tools",
                END: END
            }
        )
        
        # tools â†’ check_interrupt
        workflow.add_edge("tools", "check_interrupt")
        
        # check_interrupt â†’ agent
        workflow.add_edge("check_interrupt", "agent")

        memory = MemorySaver()
        
        return workflow.compile(checkpointer=memory)

    def chat(self, user_text: str, thread_id: str = "default_thread") -> str:
        """
        ì¼ë°˜ ì±„íŒ… ë©”ì„œë“œ
        
        Returns:
            str: AIì˜ ì‘ë‹µ ë˜ëŠ” interrupt ì •ë³´
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        result = self.graph.invoke(
            {"messages": [HumanMessage(content=user_text)]},
            config
        )
        
        # interruptê°€ ë°œìƒí•œ ê²½ìš° í™•ì¸
        if "__interrupt__" in result:
            interrupt_info = result["__interrupt__"][0].value
            return f"[INTERRUPT] {interrupt_info}"
        
        # ì •ìƒ ì‘ë‹µ
        final_response = ""
        if "messages" in result:
            last_msg = result["messages"][-1]
            if isinstance(last_msg, AIMessage):
                final_response = last_msg.content
        
        if final_response:
            extract_and_save_memory(user_text, final_response)
        
        return final_response
    
    def resume_chat(self, user_response: str, thread_id: str = "default_thread") -> str:
        """
        ì¸í„°ëŸ½íŠ¸ í›„ ìž¬ê°œ ë©”ì„œë“œ
        
        Args:
            user_response: ì‚¬ìš©ìžì˜ ì‘ë‹µ (continue ë˜ëŠ” stop)
            thread_id: ìŠ¤ë ˆë“œ ID
            
        Returns:
            str: AIì˜ ìµœì¢… ì‘ë‹µ
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        # Command(resume=...)ë¡œ ìž¬ê°œ
        result = self.graph.invoke(
            Command(resume=user_response),
            config
        )
        
        # ë˜ ë‹¤ë¥¸ interruptê°€ ë°œìƒí•œ ê²½ìš°
        if "__interrupt__" in result:
            interrupt_info = result["__interrupt__"][0].value
            return f"[INTERRUPT] {interrupt_info}"
        
        # ì •ìƒ ì‘ë‹µ
        final_response = ""
        if "messages" in result:
            last_msg = result["messages"][-1]
            if isinstance(last_msg, AIMessage):
                final_response = last_msg.content
        
        return final_response
    
    def chat_stream(self, user_text: str, thread_id: str = "default_thread") -> Generator[Dict[str, Any], None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë²„ì „
        
        Yields:
            dict: ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        final_response = ""
        interrupted = False
        
        for event in self.graph.stream(
            {"messages": [HumanMessage(content=user_text)]},
            config,
            stream_mode="updates"
        ):
            for node_name, update_value in event.items():
                
                # interrupt ì²´í¬
                if "__interrupt__" in update_value:
                    interrupted = True
                    interrupt_info = update_value["__interrupt__"][0].value
                    yield {
                        "node": node_name,
                        "type": "interrupt",
                        "content": interrupt_info
                    }
                    continue
                
                if "messages" in update_value:
                    messages = update_value["messages"]
                    
                    for msg in messages:
                        # AIMessage ì²˜ë¦¬
                        if isinstance(msg, AIMessage):
                            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                for tool_call in msg.tool_calls:
                                    yield {
                                        "node": node_name,
                                        "type": "tool_call",
                                        "tool_name": tool_call["name"],
                                        "tool_args": tool_call["args"]
                                    }
                            elif msg.content:
                                yield {
                                    "node": node_name,
                                    "type": "ai_message",
                                    "content": msg.content
                                }
                                final_response = msg.content
                        
                        # ToolMessage ì²˜ë¦¬
                        elif isinstance(msg, ToolMessage):
                            try:
                                tool_result = json.loads(msg.content)
                            except:
                                tool_result = msg.content
                            
                            yield {
                                "node": node_name,
                                "type": "tool_result",
                                "tool_name": msg.name,
                                "result": tool_result
                            }
                        
                        # SystemMessage ì²˜ë¦¬
                        elif isinstance(msg, SystemMessage):
                            yield {
                                "node": node_name,
                                "type": "system_message",
                                "content": msg.content
                            }
                
                # google_search_count ì—…ë°ì´íŠ¸
                if "google_search_count" in update_value:
                    yield {
                        "node": node_name,
                        "type": "search_count",
                        "count": update_value["google_search_count"]
                    }
        
        # interruptê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë©”ëª¨ë¦¬ ì €ìž¥
        if final_response and not interrupted:
            extract_and_save_memory(user_text, final_response)
    
    def stream_resume(self, user_response: str, thread_id: str = "default_thread") -> Generator[Dict[str, Any], None, None]:
        """
        ì¸í„°ëŸ½íŠ¸ í›„ ìž¬ê°œ ìŠ¤íŠ¸ë¦¬ë°
        
        Args:
            user_response: ì‚¬ìš©ìžì˜ ì‘ë‹µ
            thread_id: ìŠ¤ë ˆë“œ ID
            
        Yields:
            dict: ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        for event in self.graph.stream(
            Command(resume=user_response),
            config,
            stream_mode="updates"
        ):
            for node_name, update_value in event.items():
                
                # interrupt ì²´í¬
                if "__interrupt__" in update_value:
                    interrupt_info = update_value["__interrupt__"][0].value
                    yield {
                        "node": node_name,
                        "type": "interrupt",
                        "content": interrupt_info
                    }
                    continue
                
                if "messages" in update_value:
                    messages = update_value["messages"]
                    
                    for msg in messages:
                        if isinstance(msg, AIMessage):
                            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                                for tool_call in msg.tool_calls:
                                    yield {
                                        "node": node_name,
                                        "type": "tool_call",
                                        "tool_name": tool_call["name"],
                                        "tool_args": tool_call["args"]
                                    }
                            elif msg.content:
                                yield {
                                    "node": node_name,
                                    "type": "ai_message",
                                    "content": msg.content
                                }
                        
                        elif isinstance(msg, ToolMessage):
                            try:
                                tool_result = json.loads(msg.content)
                            except:
                                tool_result = msg.content
                            
                            yield {
                                "node": node_name,
                                "type": "tool_result",
                                "tool_name": msg.name,
                                "result": tool_result
                            }
                        
                        elif isinstance(msg, SystemMessage):
                            yield {
                                "node": node_name,
                                "type": "system_message",
                                "content": msg.content
                            }


def make_agent(model: str = "gpt-4o-mini") -> LangGraphAgent:
    return LangGraphAgent(model=model)