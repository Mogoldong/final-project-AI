import os
import json
from typing import TypedDict, Annotated, List, Literal
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from src.agent.tool_registry import ToolRegistry, register_default_tools
from src.agent.memory_extractor import extract_and_save_memory

load_dotenv()

'''
LangGraphAgent
- Open AIì™€ Toolì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
- ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë™ì‘
'''

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

class LangGraphAgent:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.registry = register_default_tools()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(model=model, api_key=self.api_key, temperature=0)
        self.tools_schema = self.registry.list_openai_tools()
        self.llm_with_tools = self.llm.bind_tools(self.tools_schema)

        self.system_prompt = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìƒí™©ê³¼ ê¸°ë¶„ì— ë§ì¶° ìš”ë¦¬ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” AI ì…°í”„ë´‡ì…ë‹ˆë‹¤.
        - ì‚¬ìš©ìì˜ ì·¨í–¥ì´ë‚˜ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ê¸°ì–µ(read_memory)í•˜ê³  í™œìš©í•˜ì„¸ìš”.
        - RAG(ë ˆì‹œí”¼/ì§€ì‹ ê²€ìƒ‰)ì— ì •ë³´ê°€ ì—†ê±°ë‚˜, ì¬ë£Œ ëŒ€ì²´ë²• ë“± ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ 'êµ¬ê¸€ ê²€ìƒ‰' íˆ´ì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        - í•­ìƒ ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        self.graph = self._build_graph()

    def call_model(self, state: AgentState):
        """
        í˜„ì¬ ìƒíƒœ(ë©”ì‹œì§€)ë¥¼ ë°›ì•„ LLMì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë…¸ë“œ
        """
        messages = state["messages"]
        
        if not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=self.system_prompt)] + messages
            
        response = self.llm_with_tools.invoke(messages)
        
        return {"messages": [response]}

    def run_tools(self, state: AgentState):
        """
        LLMì´ ìš”ì²­í•œ Tool Callì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë…¸ë“œ
        """
        last_message = state["messages"][-1]
        tool_calls = last_message.tool_calls
        
        results = []
        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_id = tool_call["id"]
            
            print(f"âš¡ [Graph] íˆ´ ì‹¤í–‰: {tool_name}({tool_args})")
            
            try:
                tool_output = self.registry.call(tool_name, tool_args)
            except Exception as e:
                tool_output = f"Error: {str(e)}"

            content = json.dumps(tool_output, ensure_ascii=False)
            print(f"  âœ… ê²°ê³¼: {content[:50]}...")

            results.append(ToolMessage(
                tool_call_id=tool_id,
                name=tool_name,
                content=content
            ))
            
        return {"messages": results}

    def should_continue(self, state: AgentState) -> Literal["tools", END]:
        """
        LLMì˜ ì‘ë‹µì„ ë³´ê³  íˆ´ì„ ì‹¤í–‰í• ì§€(tools) ì¢…ë£Œí• ì§€(END) ê²°ì •
        """
        last_message = state["messages"][-1]
        
        if last_message.tool_calls:
            return "tools"
        return END

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", self.run_tools)
        workflow.set_entry_point("agent")
        workflow.add_conditional_edges(
            "agent",
            self.should_continue,
            {"tools": "tools", END: END}
        )
        
        workflow.add_edge("tools", "agent")

        memory = MemorySaver()
        
        return workflow.compile(checkpointer=memory)

    def chat(self, user_text: str, thread_id: str = "default_thread") -> str:
        print(f"\nğŸ¤– [LangGraph] ì‚¬ìš©ì ì…ë ¥: {user_text}")
        
        config = {"configurable": {"thread_id": thread_id}}
        
        events = self.graph.stream(
            {"messages": [HumanMessage(content=user_text)]}, 
            config, 
            stream_mode="values"
        )
        
        final_response = ""
        for event in events:
            if "messages" in event:
                last_msg = event["messages"][-1]
                if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:
                    final_response = last_msg.content
        
        print("\nğŸ§  [Memory] ëŒ€í™” ë‚´ìš© ë¶„ì„ ë° ìë™ ì €ì¥ ì‹œë„...")
        extract_and_save_memory(user_text, final_response)
        
        return final_response


def make_agent(model: str = "gpt-4o-mini") -> LangGraphAgent:
    return LangGraphAgent(model=model)

# --- í…ŒìŠ¤íŠ¸ ì½”ë“œ ---
if __name__ == "__main__":
    agent = make_agent()
    print("ğŸ³ LangGraph ì—ì´ì „íŠ¸ ì¤€ë¹„ ì™„ë£Œ!")
    response = agent.chat("ì•ˆë…•? ë‚˜ëŠ” ì„œìš¸ì— ì‚¬ëŠ” ìˆ˜í˜„ì´ì•¼.")
    print(f"\nğŸ’¬ ë‹µë³€: {response}")
    response = agent.chat("ì˜¤ëŠ˜ ë¹„ê°€ ì˜¤ëŠ”ë° ë‚´ê°€ ì‚¬ëŠ” ê³³ ë‚ ì”¨ ì¢€ í™•ì¸í•´ì¤„ë˜?")
    print(f"\nğŸ’¬ ë‹µë³€: {response}")