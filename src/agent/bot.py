import os
import json
from typing import TypedDict, Annotated, List, Literal, Generator, Dict, Any
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from src.agent.tool_registry import ToolRegistry, register_default_tools
from src.agent.memory_extractor import extract_and_save_memory

load_dotenv()

class BaseMessage:
    """ê¸°ë³¸ ë©”ì‹œì§€ í´ëž˜ìŠ¤"""
    def __init__(self, content: str = "", **kwargs):
        self.content = content
        self.additional_kwargs = kwargs
    
    def __repr__(self):
        return f"{self.__class__.__name__}(content='{self.content}')"


class SystemMessage(BaseMessage):
    """ì‹œìŠ¤í…œ ë©”ì‹œì§€"""
    type: str = "system"


class HumanMessage(BaseMessage):
    """ì‚¬ìš©ìž ë©”ì‹œì§€"""
    type: str = "human"


class AIMessage(BaseMessage):
    """AI ì‘ë‹µ ë©”ì‹œì§€"""
    type: str = "ai"
    
    def __init__(self, content: str = "", tool_calls: List[Dict] = None, **kwargs):
        super().__init__(content, **kwargs)
        self.tool_calls = tool_calls or []


class ToolMessage(BaseMessage):
    """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë©”ì‹œì§€"""
    type: str = "tool"
    
    def __init__(self, content: str = "", tool_call_id: str = "", name: str = "", **kwargs):
        super().__init__(content, **kwargs)
        self.tool_call_id = tool_call_id
        self.name = name

# LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•œë‹¤. messagesëŠ” ëŒ€í™” ê¸°ë¡ì„, google_search_countëŠ” êµ¬ê¸€ ê²€ìƒ‰ íˆ´ ì‚¬ìš© íšŸìˆ˜ë¥¼ ì¶”ì í•œë‹¤.
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    google_search_count: int

class LangGraphAgent:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.registry = register_default_tools()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(model=model, api_key=self.api_key, temperature=0, streaming=True) # Streaming ê¸°ëŠ¥ í™œì„±í™”
        self.tools_schema = self.registry.list_openai_tools() # LLMì´ ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ë„êµ¬ ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì ¸ì˜´
        self.llm_with_tools = self.llm.bind_tools(self.tools_schema)

        self.system_prompt = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìžì˜ ìƒí™©ê³¼ ê¸°ë¶„ì— ë§žì¶° ìš”ë¦¬ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” AI ì…°í”„ë´‡ìž…ë‹ˆë‹¤.
        - ì‚¬ìš©ìžì˜ ì·¨í–¥ì´ë‚˜ ì•Œë ˆë¥´ê¸° ì •ë³´ë¥¼ ê¸°ì–µ(read_memory)í•˜ê³  í™œìš©í•˜ì„¸ìš”.
        - RAG(ë ˆì‹œí”¼/ì§€ì‹ ê²€ìƒ‰)ì— ì •ë³´ê°€ ì—†ê±°ë‚˜, ìž¬ë£Œ ëŒ€ì²´ë²• ë“± ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ìžˆìœ¼ë©´ 'êµ¬ê¸€ ê²€ìƒ‰' íˆ´ì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
        - í•­ìƒ ì¹œì ˆí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        ** ì¤‘ìš”: Google ê²€ìƒ‰ íšŸìˆ˜ê°€ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´, ì‹œìŠ¤í…œì´ ê²½ê³  ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤. 
        ì´ë•Œ ì‚¬ìš©ìžê°€ 'ë„¤', 'ê³„ì†', 'yes' ë“±ìœ¼ë¡œ ë‹µë³€í•˜ë©´ ê²€ìƒ‰ì„ ê³„ì† ì§„í–‰í•˜ê³ ,
        ê·¸ ì™¸ì˜ ë‹µë³€ì´ë©´ ê²€ìƒ‰ ì—†ì´ í˜„ìž¬ ì •ë³´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """
        
        self.graph = self._build_graph()

    # Agent ë…¸ë“œë¡œ í˜„ìž¬ ìƒíƒœì—ì„œ ë©”ì„¸ì§€ë¥¼ LLMì— ì „ë‹¬í•˜ê³  ì‘ë‹µ(response)ì„ ë°›ëŠ”ë‹¤.
    def call_model(self, state: AgentState):
        messages = state["messages"]
        
        if not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=self.system_prompt)] + messages
            
        response = self.llm_with_tools.invoke(messages)
        
        return {"messages": [response]}

    # LLMì´ ìš”ì²­í•œ íˆ´ì—ì„œ name, args, idë¥¼ ì¶”ì¶œí•˜ì—¬ ì‹¤í–‰í•˜ê³  ToolMessage í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤. ì´ ê³¼ì •ì—ì„œ google_search_countë„ ì—…ë°ì´íŠ¸í•œë‹¤.
    def run_tools(self, state: AgentState):
        last_message = state["messages"][-1]
        tool_calls = last_message.tool_calls
        
        results = []
        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_id = tool_call["id"]
            
            try:
                tool_output = self.registry.call(tool_name, tool_args)
            except Exception as e:
                tool_output = f"Error: {str(e)}"

            content = json.dumps(tool_output, ensure_ascii=False)

            results.append(ToolMessage(
                tool_call_id=tool_id,
                name=tool_name,
                content=content
            ))
            
        google_search_count = state.get("google_search_count", 0)
        search_count_in_turn = sum(1 for msg in results if msg.name == 'search_google')
        
        return {"messages": results, "google_search_count": google_search_count + search_count_in_turn}

    # Agent ë…¸ë“œì˜ ë‹¤ìŒì„ ê²°ì •í•œë‹¤. ì¸í„°ëŸ½íŠ¸ ë°œìƒì´ë‚˜ tool í˜¸ì¶œ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°í•œë‹¤. 
    def should_continue(self, state: AgentState) -> Literal["tools", END]:
        last_message = state["messages"][-1]

        if isinstance(last_message, SystemMessage) and "ì¸í„°ëŸ½íŠ¸ ë°œìƒ" in last_message.content:
            return END
        
        if last_message.tool_calls:
            return "tools"
        return END
    
    # chkeck_interrupt ë…¸ë“œì˜ ë‹¤ìŒì„ ê²°ì •í•œë‹¤. ì¸í„°ëŸ½íŠ¸ ë©”ì„¸ì§€ê°€ ìžˆë‹¤ë©´ ì¢…ë£Œí•˜ê³  ì•„ë‹ˆë¼ë©´ ê³„ì† ì§„í–‰í•œë‹¤.
    def should_loop(self, state: AgentState) -> Literal["loop", END]:
        last_message = state["messages"][-1]
        
        if isinstance(last_message, SystemMessage) and "ì¸í„°ëŸ½íŠ¸ ë°œìƒ" in last_message.content:
            return END
        
        return "loop"

    # ê·¸ëž˜í”„ êµ¬ì¶•
    def _build_graph(self):
        workflow = StateGraph(AgentState)

        # ì¶”ë¡ , ë„êµ¬ì‹¤í–‰, ì¸í„°ëŸ½íŠ¸ í™•ì¸ 3ê°€ì§€ ë…¸ë“œ êµ¬í˜„
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", self.run_tools)
        workflow.add_node("check_interrupt", self.check_interrupt)

        workflow.set_entry_point("agent")
        workflow.add_conditional_edges(
            "agent",
            self.should_continue,
            {"tools": "tools", END: END}
        ) # LLMì´ ë„êµ¬ í˜¸ì¶œì„ í–ˆëŠëƒì— ë”°ë¼ tools ë…¸ë“œë¡œ ê°ˆì§€ ENDë¡œ ê°ˆì§€ ê²°ì •í•˜ëŠ” ë¶„ê¸° ë¡œì§
        
        workflow.add_edge("tools", "check_interrupt")

        workflow.add_conditional_edges(
            "check_interrupt",
            self.should_loop,
            {
                "loop": "agent",
                END: END,
            } # ì¸í„°ëŸ½íŠ¸ ë°œìƒ í›„ ê³„ì†í• ì§€ ì¢…ë£Œí• ì§€ ê²°ì •í•˜ëŠ” ë¶„ê¸° ë¡œì§
        )

        memory = MemorySaver() # ì²´í¬í¬ì¸í„°ë¡œ ì„¤ì •í•˜ì—¬ messagesì™€ google_search_count ë¥¼ thread id ë³„ë¡œ ì €ìž¥
        
        return workflow.compile(checkpointer=memory)

    # ê¸°ì¡´ ë²„ì „
    def chat(self, user_text: str, thread_id: str = "default_thread") -> str:
        config = {"configurable": {"thread_id": thread_id}}
        
        events = self.graph.stream(
            {"messages": [HumanMessage(content=user_text)]},
            config, 
            stream_mode="values"
        )
        
        final_response = ""
        for event in events:
            if "messages" in event:
                last_msg = event["messages"][-1]
                if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:
                    final_response = last_msg.content
                elif isinstance(last_msg, SystemMessage) and "ì¸í„°ëŸ½íŠ¸ ë°œìƒ" in last_msg.content:
                    final_response = last_msg.content
        
        extract_and_save_memory(user_text, final_response)
        
        return final_response
    
    # app.pyì˜ handle_message_streamì—ì„œ ìš”êµ¬í•˜ëŠ” ì‹¤ì‹œê°„ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ë©”ì„œë“œ
    def chat_stream(self, user_text: str, thread_id: str = "default_thread") -> Generator[Dict[str, Any], None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ - ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
        
        Returns:
            Generator yielding dictionaries with:
            - node: ë…¸ë“œ ì´ë¦„
            - type: ë©”ì‹œì§€ íƒ€ìž… (ai_message, tool_call, system_message ë“±)
            - content: ë©”ì‹œì§€ ë‚´ìš©
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        final_response = ""
        
        # stream_mode="updates"ë¡œ ê° ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ë¥¼ ë°›ìŒ
        for event in self.graph.stream(
            {"messages": [HumanMessage(content=user_text)]},
            config,
            stream_mode="updates"
        ):
            # eventëŠ” {node_name: update_value} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
            for node_name, update_value in event.items():
                
                # messagesê°€ ì—…ë°ì´íŠ¸ëœ ê²½ìš°
                if "messages" in update_value:
                    messages = update_value["messages"]
                    
                    for msg in messages:
                        # AIMessage ì²˜ë¦¬
                        if isinstance(msg, AIMessage):
                            if msg.tool_calls:
                                # ë„êµ¬ í˜¸ì¶œ
                                for tool_call in msg.tool_calls:
                                    yield {
                                        "node": node_name,
                                        "type": "tool_call",
                                        "tool_name": tool_call["name"],
                                        "tool_args": tool_call["args"]
                                    }
                            elif msg.content:
                                # ì¼ë°˜ AI ì‘ë‹µ
                                yield {
                                    "node": node_name,
                                    "type": "ai_message",
                                    "content": msg.content
                                }
                                final_response = msg.content
                        
                        # ToolMessage ì²˜ë¦¬
                        elif isinstance(msg, ToolMessage):
                            try:
                                tool_result = json.loads(msg.content)
                            except:
                                tool_result = msg.content
                            
                            yield {
                                "node": node_name,
                                "type": "tool_result",
                                "tool_name": msg.name,
                                "result": tool_result
                            }
                        
                        # SystemMessage ì²˜ë¦¬ (ì¸í„°ëŸ½íŠ¸ ë©”ì‹œì§€)
                        elif isinstance(msg, SystemMessage):
                            if "ì¸í„°ëŸ½íŠ¸ ë°œìƒ" in msg.content or "ì•Œë¦¼" in msg.content:
                                yield {
                                    "node": node_name,
                                    "type": "system_message",
                                    "content": msg.content
                                }
                                final_response = msg.content
                
                # google_search_count ì—…ë°ì´íŠ¸
                if "google_search_count" in update_value:
                    yield {
                        "node": node_name,
                        "type": "search_count",
                        "count": update_value["google_search_count"]
                    }
        
        # ë©”ëª¨ë¦¬ ì €ìž¥
        if final_response:
            extract_and_save_memory(user_text, final_response)
    
    def check_interrupt(self, state: AgentState):
        current_count = state.get("google_search_count", 0)
        
        if current_count >= 4:
            interrupt_message = SystemMessage(
                content=f"ðŸš¨ [ì•Œë¦¼] Google ê²€ìƒ‰ íˆ´ì„ ê¶Œìž¥ í•œë„(3íšŒ)ë¥¼ ì´ˆê³¼í•˜ì—¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. "
                    f"í•˜ë£¨ API í˜¸ì¶œ í•œë„ëŠ” 100íšŒìž…ë‹ˆë‹¤. (í˜„ìž¬ {current_count}íšŒ ì‚¬ìš©)\n\n"
                    f"ê·¸ëž˜ë„ ê³„ì† ê²€ìƒ‰ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? "
                    f"ê³„ì†í•˜ë ¤ë©´ 'ë„¤' ë˜ëŠ” 'ê³„ì†'ì´ë¼ê³  ìž…ë ¥í•´ì£¼ì„¸ìš”. "
                    f"ì¤‘ë‹¨í•˜ë ¤ë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            )
            return {"messages": [interrupt_message]}
    
        return {"messages": []}


def make_agent(model: str = "gpt-4o-mini") -> LangGraphAgent:
    return LangGraphAgent(model=model)